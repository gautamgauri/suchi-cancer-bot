# Phase 2 - Zero Citation Cases Analysis

**Date:** 2026-01-21 02:15 UTC  
**Analysis:** 9 cases with 0 citations out of 15 total

---

## Summary Table

| Case ID | Intent | Evidence Gate | Path Taken | Chunks | Citations | Abstention | Root Cause |
|---------|--------|---------------|------------|--------|-----------|------------|------------|
| **GROUP 1: LLM Generation Failure (4 cases)** |
| LUNG-GEN-01 | GEN | Passed (0.42 sim) | RAGâ†’LLMâ†’Extractâ†’Fail | 6 | 0 | citation_validation_failed | **LLM didn't generate citations** |
| LUNG-POST-01 | POST | Passed (0.42 sim) | RAGâ†’LLMâ†’Extractâ†’Fail | 6 | 0 | citation_validation_failed | **LLM didn't generate citations** |
| BREAST-GEN-01 | GEN | Passed (0.42 sim) | RAGâ†’LLMâ†’Extractâ†’Fail | 6 | 0 | citation_validation_failed | **LLM didn't generate citations** |
| ORAL-GEN-01 | GEN | Passed (0.42 sim) | RAGâ†’LLMâ†’Extractâ†’Fail | 6 | 0 | citation_validation_failed | **LLM didn't generate citations** |
| **GROUP 2: Legitimate Clarifying Questions (5 cases)** |
| BREAST-PATIENT-01 | PATIENT | Passed (0.33 sim) | RAGâ†’Template (no LLM) | 6 | 0 | none | **Clarifying question (expected)** |
| BREAST-CAREGIVER-01 | CAREGIVER | Passed (0.34 sim) | RAGâ†’Template (no LLM) | 6 | 0 | none | **Clarifying question (expected)** |
| BREAST-POST-01 | POST | Passed (0.35 sim) | RAGâ†’Template (no LLM) | 6 | 0 | none | **Clarifying question (expected)** |
| ORAL-PATIENT-01 | PATIENT | Passed (0.35 sim) | RAGâ†’Template (no LLM) | 6 | 0 | none | **Clarifying question (expected)** |
| ORAL-CAREGIVER-01 | CAREGIVER | Passed (0.33 sim) | RAGâ†’Template (no LLM) | 6 | 0 | none | **Clarifying question (expected)** |

---

## Detailed Analysis by Group

### GROUP 1: LLM Generation Failure (4 cases - 27%)

**Root Cause:** LLM called but failed to generate citations despite having evidence chunks

**Path Taken:**
1. RAG retrieval: âœ… Succeeded (6 chunks, 0.39-0.42 similarity, all trusted)
2. Evidence gate: âœ… Passed (chunks available, trusted sources)
3. LLM called: âœ… Gemini generated response
4. Citation extraction: âŒ **0 citations found in LLM response**
5. Runtime citation enforcement: âŒ Blocked (need 2+ citations for medical content)
6. Final response: SafeFallbackResponse (non-medical)

**Example Response:**
```
"I don't have enough specific information in my knowledge base to answer this accurately.
For personalized medical guidance, please consult with your healthcare provider..."
```

**Evidence:**
- All have `abstentionReason: "citation_validation_failed"`
- All have 6 retrievedChunks with trusted sources (02_nci_core)
- Similarity scores: 0.39-0.42 (decent quality)
- Response text indicates LLM was called but citations missing

**Issue Type:** **LLM Prompt/Generation Failure**

**Why This Happens:**
1. **LLM prompt not emphasizing citations strongly enough**
2. **Query type classification issue** - may be classifying as "general overview" vs "medical explanation"
3. **Content mismatch** - Retrieved chunks about treatment, query asks about identification
4. **LLM safety behavior** - Gemini may be declining to cite when uncertain

**Impact:**
- 27% of cases (4/15)
- **All GEN intents** - "How do you identify X cancer?" queries
- Retrieval working, LLM failing

---

### GROUP 2: Legitimate Clarifying Questions (5 cases - 33%)

**Root Cause:** System correctly asking for more information before providing medical content

**Path Taken:**
1. RAG retrieval: âœ… Succeeded (6 chunks, 0.33-0.35 similarity)
2. Evidence gate: âš ï¸ Likely "weak" or "needs clarification"
3. Intent classification: "MISSING_CONTEXT" or similar
4. Response: Clarifying question template (no LLM call)
5. Final response: Non-medical question (0 citations expected)

**Example Responses:**
```
"I understand you have questions. To help you better, could you share:
â€¢ What specific aspect would you like help with?"
```

```
"I can help you prepare questions about your report, but I can't interpret medical reports, scans, or test results directly."
```

**Evidence:**
- No `abstentionReason` (not an abstention, it's a clarification)
- All have 6 retrievedChunks (RAG ran)
- Similarity scores: 0.33-0.35 (lower than Group 1)
- Response text is clearly non-medical, asking for clarification

**Issue Type:** **Expected Behavior (not a bug)**

**Why This Happens:**
1. **Underspecified queries** - "I have a symptom" without details
2. **System correctly asking for clarification** before providing medical info
3. **Conversation flow design** - multi-turn expected

**Impact:**
- 33% of cases (5/15)
- **All PATIENT/CAREGIVER/POST intents**
- This is CORRECT behavior per trust-first contract

---

## Root Cause Breakdown

| Root Cause | Count | % | Acceptable? | Next Action |
|------------|-------|---|-------------|-------------|
| **LLM not generating citations** | 4 | 27% | âŒ NO | **Fix LLM prompt** |
| **Clarifying questions (expected)** | 5 | 33% | âœ… YES | Document as policy |
| **With citations (working)** | 6 | 40% | âœ… YES | - |
| **Total** | 15 | 100% | - | - |

**Actionable citation coverage:**
- Remove clarifying questions: 6 citations / 10 medical responses = **60%**
- If LLM generation fixed: 10 citations / 10 medical responses = **100%**

---

## Impact on "Production Ready" Claim

### What is Actually Achieved

âœ… **Orchestration contract: 100% complete**
- All paths return retrievedChunks
- All paths include trusted sources in top-3
- No wiring bugs, no routing bypasses

âœ… **Trust-first contract: Enforced**
- No medical content without proper citations OR clarification
- Runtime citation enforcement working (blocks 0-citation medical responses)
- Clarifying questions appropriately non-medical

âš ï¸ **Answer coverage: Limited (40% full answers, 33% clarifications, 27% blocked)**
- Only 40% of queries get full medical answers with citations
- 33% correctly ask for clarification (multi-turn expected)
- 27% blocked due to LLM generation failure

### Accurate Milestone Assessment

**Phase 2 Status:** âœ… **"Trust-First Orchestration Complete"**

**NOT:** "Production-ready for broad public use"  
**IS:** "Production-ready for pilot with known limitations"

**Accurate description:**
> "Phase 2 successfully established complete evaluation observability (100% retrievedChunks coverage) and consistent trust-first contract enforcement across all response paths. System correctly routes all queries and maintains metadata integrity. However, answer coverage remains limited (40% citation rate) due to LLM prompt issues causing generation failures for general overview queries. Clarifying question flow accounts for 33% of responses (expected behavior). Phase 3 will address LLM prompt optimization and potentially KB expansion."

---

## Recommended Actions (Priority Order)

### CRITICAL: Fix LLM Prompt (Quick Win - 1 hour)

**Impact:** 27% â†’ 0% generation failures, 40% â†’ 67% citation coverage

**Root Cause:**
GEN intent queries ("How do you identify lung cancer?") are triggering LLM to generate overview responses WITHOUT citations.

**Fix:**
```typescript
// In llm.service.ts, generateWithCitations method
// For GEN intent, use stronger citation enforcement prompt:

if (queryType === 'general' || intent === 'INFORMATIONAL_GENERAL') {
  systemPrompt += `
  
CRITICAL: You MUST include citations for ALL factual medical information.
Use the provided sources to answer. Include [citation:sourceId:chunkId] 
for EVERY sentence that contains medical facts.

Example:
"Lung cancer is diagnosed through imaging tests [citation:doc1:chunk1] 
and biopsy [citation:doc1:chunk2]."
`;
}
```

**Expected Outcome:**
- LUNG-GEN-01: 0 â†’ 3-5 citations
- LUNG-POST-01: 0 â†’ 3-5 citations
- BREAST-GEN-01: 0 â†’ 3-5 citations
- ORAL-GEN-01: 0 â†’ 3-5 citations

### HIGH: Document Clarifying Question Policy (30 min)

**Impact:** Clarifies evaluation expectations

**Action:**
Update evaluation rubric to distinguish:
- **Medical response** â†’ Requires 2+ citations
- **Clarifying question** â†’ 0 citations expected, marked as "needs_more_info"
- **Pure navigation** â†’ 0 citations expected, marked as "non_medical"

Add field to response: `responseCategory: "medical" | "clarification" | "navigation"`

### MEDIUM: Add Regression Gates (2 hours)

**Prevent Phase 2 regressions:**

1. **CI/Nightly check:**
```yaml
# In evaluation config
regression_gates:
  - name: "retrievedChunks_coverage"
    threshold: 1.0
    fail_on_miss: true
    
  - name: "medical_response_has_citations"
    query: "responseCategory == 'medical' AND citations.length < 2"
    threshold: 0
    fail_on_miss: true
```

2. **Structured logging per request:**
```typescript
logger.info({
  event: 'response_complete',
  intent,
  pathTaken, // "rag_llm_success" | "rag_llm_abstain" | "template_clarify"
  retrievedChunksCount,
  citationsCount,
  top3TrustedCount,
  evidenceGateStatus,
  abstentionReason,
  responseCategory // "medical" | "clarification" | "navigation"
});
```

3. **Dashboard metrics:**
- Citation rate by intent type
- Abstention reasons breakdown
- retrievedChunks coverage (should stay 100%)

### LOW: Consider KB Expansion (Phase 3)

**Current state:** NCI-only limits coverage for some query types

**Recommendation:** Add 1-2 Tier-1 sources AFTER fixing LLM prompt
- Fix prompt first (bigger impact for zero cost)
- Then add CRUK/Macmillan if coverage still low

---

## Revised Success Criteria

### Phase 2 (Current) - COMPLETE âœ…

- âœ… retrievedChunks in 100% of responses
- âœ… Top-3 trusted â‰¥90% (achieved 100%)
- âœ… Trust-first contract enforced
- âœ… No orchestration regressions
- âš ï¸ Citation coverage 40% (acceptable with clarifications, BUT LLM issue identified)

### Phase 2.5 (Quick Fix) - Recommended

- ðŸŽ¯ Fix LLM prompt for GEN intent citations
- ðŸŽ¯ Document clarifying question policy
- ðŸŽ¯ Add regression gates
- **Target:** Citation coverage 60-70% (accounting for legitimate clarifications)

### Phase 3 (Future) - Optional

- KB expansion (CRUK, Macmillan)
- Query expansion for symptom queries
- Intent-sensitive evidence thresholds

---

## The Honest Funder/Partner Message

**What we achieved in Phase 2:**
> "We successfully resolved the 'trust paradox' where evaluation metrics appeared inconsistent with our 100% trusted KB. Root cause was incomplete metadata in 50% of response paths. Phase 2 fixed all orchestration paths, achieving 100% retrievedChunks coverage and 100% top-3 trusted source presence. This provides complete evaluation visibility and confirms our trust-aware retrieval is working as designed."

**What we discovered:**
> "With complete visibility, we identified an LLM prompt issue affecting 27% of queries (general overview questions). The system correctly retrieves trusted sources but Gemini fails to generate citations for these query types, triggering our safety fallback. Additionally, 33% of queries correctly ask for clarification before providing medical content (multi-turn conversation design). Net result: 40% full medical answers with citations on first turn."

**What's next:**
> "Phase 2.5 (1-2 day fix): Optimize LLM prompt to enforce citations for all query types. Expected improvement: 40% â†’ 67% citation coverage. Phase 3 (optional): KB expansion with CRUK/Macmillan to improve coverage breadth."

**Current recommendation:**
> "System is ready for controlled pilot with known limitations documented. Users may need to rephrase queries or provide additional context for ~30% of questions. All medical content that IS returned includes proper citations from trusted sources."

---

**Analysis Complete**  
**Next Action:** Fix LLM prompt for GEN intent (highest ROI)
